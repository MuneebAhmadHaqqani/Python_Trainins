{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(r\"C:\\Users\\etisalat\\deep learning\\reference\\TF_2_Notebooks_and_Data\\DATA\\iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.drop(\"species\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=iris[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y= encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(iris[\"species\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "# either we apply (fit) and then (transform) or we can only apply (fit_transform) \n",
    "#scaler.fit(x_train)\n",
    "#scaler.transform(s_train)\n",
    "\n",
    "# so (fit_transform) is doign the job of both (fit) and (transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x_train=scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x_test=scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "model.add(Dense(units=3,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop=EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/250\n",
      "120/120 [==============================] - 2s 13ms/sample - loss: 1.2391 - accuracy: 0.2917 - val_loss: 1.1992 - val_accuracy: 0.3000\n",
      "Epoch 2/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 1.2323 - accuracy: 0.2917 - val_loss: 1.1945 - val_accuracy: 0.3000\n",
      "Epoch 3/250\n",
      "120/120 [==============================] - 0s 850us/sample - loss: 1.2251 - accuracy: 0.2917 - val_loss: 1.1900 - val_accuracy: 0.3000\n",
      "Epoch 4/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.2181 - accuracy: 0.2917 - val_loss: 1.1857 - val_accuracy: 0.3000\n",
      "Epoch 5/250\n",
      "120/120 [==============================] - 0s 558us/sample - loss: 1.2128 - accuracy: 0.2917 - val_loss: 1.1818 - val_accuracy: 0.2667\n",
      "Epoch 6/250\n",
      "120/120 [==============================] - 0s 850us/sample - loss: 1.2059 - accuracy: 0.2917 - val_loss: 1.1780 - val_accuracy: 0.2667\n",
      "Epoch 7/250\n",
      "120/120 [==============================] - 0s 833us/sample - loss: 1.2009 - accuracy: 0.2917 - val_loss: 1.1746 - val_accuracy: 0.2667\n",
      "Epoch 8/250\n",
      "120/120 [==============================] - 0s 783us/sample - loss: 1.1958 - accuracy: 0.2917 - val_loss: 1.1715 - val_accuracy: 0.2667\n",
      "Epoch 9/250\n",
      "120/120 [==============================] - 0s 992us/sample - loss: 1.1916 - accuracy: 0.2917 - val_loss: 1.1686 - val_accuracy: 0.2667\n",
      "Epoch 10/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1864 - accuracy: 0.2833 - val_loss: 1.1658 - val_accuracy: 0.2667\n",
      "Epoch 11/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 1.1822 - accuracy: 0.2750 - val_loss: 1.1632 - val_accuracy: 0.2667\n",
      "Epoch 12/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1783 - accuracy: 0.2750 - val_loss: 1.1608 - val_accuracy: 0.2667\n",
      "Epoch 13/250\n",
      "120/120 [==============================] - 0s 675us/sample - loss: 1.1746 - accuracy: 0.2750 - val_loss: 1.1586 - val_accuracy: 0.2667\n",
      "Epoch 14/250\n",
      "120/120 [==============================] - 0s 567us/sample - loss: 1.1706 - accuracy: 0.2667 - val_loss: 1.1564 - val_accuracy: 0.2667\n",
      "Epoch 15/250\n",
      "120/120 [==============================] - 0s 717us/sample - loss: 1.1673 - accuracy: 0.2583 - val_loss: 1.1545 - val_accuracy: 0.2667\n",
      "Epoch 16/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1637 - accuracy: 0.2500 - val_loss: 1.1526 - val_accuracy: 0.2667\n",
      "Epoch 17/250\n",
      "120/120 [==============================] - 0s 692us/sample - loss: 1.1604 - accuracy: 0.2500 - val_loss: 1.1508 - val_accuracy: 0.2667\n",
      "Epoch 18/250\n",
      "120/120 [==============================] - 0s 675us/sample - loss: 1.1572 - accuracy: 0.2500 - val_loss: 1.1491 - val_accuracy: 0.2667\n",
      "Epoch 19/250\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 1.1544 - accuracy: 0.2500 - val_loss: 1.1475 - val_accuracy: 0.2667\n",
      "Epoch 20/250\n",
      "120/120 [==============================] - 0s 725us/sample - loss: 1.1517 - accuracy: 0.2500 - val_loss: 1.1461 - val_accuracy: 0.2333\n",
      "Epoch 21/250\n",
      "120/120 [==============================] - 0s 825us/sample - loss: 1.1490 - accuracy: 0.2417 - val_loss: 1.1448 - val_accuracy: 0.2000\n",
      "Epoch 22/250\n",
      "120/120 [==============================] - 0s 933us/sample - loss: 1.1465 - accuracy: 0.2417 - val_loss: 1.1435 - val_accuracy: 0.2000\n",
      "Epoch 23/250\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 1.1443 - accuracy: 0.2333 - val_loss: 1.1424 - val_accuracy: 0.2000\n",
      "Epoch 24/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1421 - accuracy: 0.2250 - val_loss: 1.1414 - val_accuracy: 0.2000\n",
      "Epoch 25/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1396 - accuracy: 0.2167 - val_loss: 1.1403 - val_accuracy: 0.2000\n",
      "Epoch 26/250\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 1.1376 - accuracy: 0.1917 - val_loss: 1.1393 - val_accuracy: 0.2000\n",
      "Epoch 27/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.1355 - accuracy: 0.1750 - val_loss: 1.1383 - val_accuracy: 0.1333\n",
      "Epoch 28/250\n",
      "120/120 [==============================] - 0s 892us/sample - loss: 1.1337 - accuracy: 0.1667 - val_loss: 1.1375 - val_accuracy: 0.1333\n",
      "Epoch 29/250\n",
      "120/120 [==============================] - 0s 675us/sample - loss: 1.1319 - accuracy: 0.1500 - val_loss: 1.1366 - val_accuracy: 0.1333\n",
      "Epoch 30/250\n",
      "120/120 [==============================] - 0s 858us/sample - loss: 1.1299 - accuracy: 0.1333 - val_loss: 1.1357 - val_accuracy: 0.1333\n",
      "Epoch 31/250\n",
      "120/120 [==============================] - 0s 625us/sample - loss: 1.1285 - accuracy: 0.1250 - val_loss: 1.1349 - val_accuracy: 0.1333\n",
      "Epoch 32/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 1.1270 - accuracy: 0.1667 - val_loss: 1.1342 - val_accuracy: 0.1667\n",
      "Epoch 33/250\n",
      "120/120 [==============================] - 0s 958us/sample - loss: 1.1252 - accuracy: 0.1583 - val_loss: 1.1334 - val_accuracy: 0.1667\n",
      "Epoch 34/250\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1238 - accuracy: 0.1833 - val_loss: 1.1326 - val_accuracy: 0.1667\n",
      "Epoch 35/250\n",
      "120/120 [==============================] - 0s 725us/sample - loss: 1.1221 - accuracy: 0.2167 - val_loss: 1.1318 - val_accuracy: 0.1667\n",
      "Epoch 36/250\n",
      "120/120 [==============================] - 0s 783us/sample - loss: 1.1206 - accuracy: 0.2333 - val_loss: 1.1307 - val_accuracy: 0.2000\n",
      "Epoch 37/250\n",
      "120/120 [==============================] - 0s 800us/sample - loss: 1.1185 - accuracy: 0.2917 - val_loss: 1.1294 - val_accuracy: 0.2000\n",
      "Epoch 38/250\n",
      "120/120 [==============================] - 0s 692us/sample - loss: 1.1164 - accuracy: 0.3083 - val_loss: 1.1278 - val_accuracy: 0.2000\n",
      "Epoch 39/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.1139 - accuracy: 0.3167 - val_loss: 1.1259 - val_accuracy: 0.2000\n",
      "Epoch 40/250\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 1.1113 - accuracy: 0.3167 - val_loss: 1.1240 - val_accuracy: 0.2333\n",
      "Epoch 41/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.1090 - accuracy: 0.3333 - val_loss: 1.1225 - val_accuracy: 0.2333\n",
      "Epoch 42/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 1.1071 - accuracy: 0.3333 - val_loss: 1.1212 - val_accuracy: 0.2333\n",
      "Epoch 43/250\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 1.1057 - accuracy: 0.3250 - val_loss: 1.1203 - val_accuracy: 0.2333\n",
      "Epoch 44/250\n",
      "120/120 [==============================] - 0s 617us/sample - loss: 1.1049 - accuracy: 0.3250 - val_loss: 1.1196 - val_accuracy: 0.2333\n",
      "Epoch 45/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 1.1044 - accuracy: 0.3250 - val_loss: 1.1188 - val_accuracy: 0.2667\n",
      "Epoch 46/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.1039 - accuracy: 0.3250 - val_loss: 1.1183 - val_accuracy: 0.2667\n",
      "Epoch 47/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.1034 - accuracy: 0.3250 - val_loss: 1.1178 - val_accuracy: 0.2667\n",
      "Epoch 48/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.1030 - accuracy: 0.3333 - val_loss: 1.1173 - val_accuracy: 0.2667\n",
      "Epoch 49/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.1025 - accuracy: 0.3333 - val_loss: 1.1169 - val_accuracy: 0.2667\n",
      "Epoch 50/250\n",
      "120/120 [==============================] - 0s 600us/sample - loss: 1.1021 - accuracy: 0.3333 - val_loss: 1.1165 - val_accuracy: 0.2667\n",
      "Epoch 51/250\n",
      "120/120 [==============================] - 0s 650us/sample - loss: 1.1016 - accuracy: 0.3333 - val_loss: 1.1162 - val_accuracy: 0.2667\n",
      "Epoch 52/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.1013 - accuracy: 0.3333 - val_loss: 1.1159 - val_accuracy: 0.2333\n",
      "Epoch 53/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.1008 - accuracy: 0.3333 - val_loss: 1.1155 - val_accuracy: 0.2667\n",
      "Epoch 54/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.1004 - accuracy: 0.3333 - val_loss: 1.1151 - val_accuracy: 0.2667\n",
      "Epoch 55/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.1000 - accuracy: 0.3333 - val_loss: 1.1147 - val_accuracy: 0.2667\n",
      "Epoch 56/250\n",
      "120/120 [==============================] - 0s 625us/sample - loss: 1.0996 - accuracy: 0.3333 - val_loss: 1.1143 - val_accuracy: 0.2667\n",
      "Epoch 57/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 1.0992 - accuracy: 0.3250 - val_loss: 1.1140 - val_accuracy: 0.2667\n",
      "Epoch 58/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0988 - accuracy: 0.3167 - val_loss: 1.1137 - val_accuracy: 0.3000\n",
      "Epoch 59/250\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 1.0983 - accuracy: 0.3250 - val_loss: 1.1133 - val_accuracy: 0.3000\n",
      "Epoch 60/250\n",
      "120/120 [==============================] - 0s 767us/sample - loss: 1.0979 - accuracy: 0.3250 - val_loss: 1.1130 - val_accuracy: 0.3000\n",
      "Epoch 61/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 1.0975 - accuracy: 0.3250 - val_loss: 1.1126 - val_accuracy: 0.3333\n",
      "Epoch 62/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0971 - accuracy: 0.3250 - val_loss: 1.1122 - val_accuracy: 0.3333\n",
      "Epoch 63/250\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 1.0966 - accuracy: 0.3250 - val_loss: 1.1119 - val_accuracy: 0.3667\n",
      "Epoch 64/250\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 1.0962 - accuracy: 0.3167 - val_loss: 1.1115 - val_accuracy: 0.4000\n",
      "Epoch 65/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0958 - accuracy: 0.3333 - val_loss: 1.1112 - val_accuracy: 0.4000\n",
      "Epoch 66/250\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 1.0953 - accuracy: 0.3417 - val_loss: 1.1108 - val_accuracy: 0.4000\n",
      "Epoch 67/250\n",
      "120/120 [==============================] - 0s 708us/sample - loss: 1.0949 - accuracy: 0.3417 - val_loss: 1.1105 - val_accuracy: 0.4000\n",
      "Epoch 68/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0944 - accuracy: 0.3417 - val_loss: 1.1101 - val_accuracy: 0.4000\n",
      "Epoch 69/250\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.0916 - accuracy: 0.43 - 0s 575us/sample - loss: 1.0940 - accuracy: 0.3750 - val_loss: 1.1097 - val_accuracy: 0.4000\n",
      "Epoch 70/250\n",
      "120/120 [==============================] - 0s 650us/sample - loss: 1.0935 - accuracy: 0.3833 - val_loss: 1.1093 - val_accuracy: 0.4000\n",
      "Epoch 71/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 1.0931 - accuracy: 0.3917 - val_loss: 1.1089 - val_accuracy: 0.4000\n",
      "Epoch 72/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0926 - accuracy: 0.3917 - val_loss: 1.1085 - val_accuracy: 0.4000\n",
      "Epoch 73/250\n",
      "120/120 [==============================] - 0s 692us/sample - loss: 1.0921 - accuracy: 0.4000 - val_loss: 1.1082 - val_accuracy: 0.3667\n",
      "Epoch 74/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.0916 - accuracy: 0.4167 - val_loss: 1.1078 - val_accuracy: 0.3667\n",
      "Epoch 75/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.0911 - accuracy: 0.4250 - val_loss: 1.1073 - val_accuracy: 0.3667\n",
      "Epoch 76/250\n",
      "120/120 [==============================] - 0s 733us/sample - loss: 1.0906 - accuracy: 0.4333 - val_loss: 1.1069 - val_accuracy: 0.3667\n",
      "Epoch 77/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 1.0901 - accuracy: 0.4333 - val_loss: 1.1066 - val_accuracy: 0.4000\n",
      "Epoch 78/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0895 - accuracy: 0.4333 - val_loss: 1.1061 - val_accuracy: 0.4000\n",
      "Epoch 79/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0890 - accuracy: 0.4417 - val_loss: 1.1057 - val_accuracy: 0.4000\n",
      "Epoch 80/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0884 - accuracy: 0.4417 - val_loss: 1.1051 - val_accuracy: 0.4000\n",
      "Epoch 81/250\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0879 - accuracy: 0.4417 - val_loss: 1.1047 - val_accuracy: 0.4000\n",
      "Epoch 82/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.0873 - accuracy: 0.4500 - val_loss: 1.1042 - val_accuracy: 0.4000\n",
      "Epoch 83/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.0867 - accuracy: 0.4583 - val_loss: 1.1037 - val_accuracy: 0.4000\n",
      "Epoch 84/250\n",
      "120/120 [==============================] - 0s 725us/sample - loss: 1.0861 - accuracy: 0.4667 - val_loss: 1.1033 - val_accuracy: 0.4000\n",
      "Epoch 85/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 1.0855 - accuracy: 0.4667 - val_loss: 1.1028 - val_accuracy: 0.4000\n",
      "Epoch 86/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0849 - accuracy: 0.4833 - val_loss: 1.1022 - val_accuracy: 0.4000\n",
      "Epoch 87/250\n",
      "120/120 [==============================] - 0s 675us/sample - loss: 1.0842 - accuracy: 0.4917 - val_loss: 1.1017 - val_accuracy: 0.4000\n",
      "Epoch 88/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 1.0836 - accuracy: 0.5000 - val_loss: 1.1011 - val_accuracy: 0.4000\n",
      "Epoch 89/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.0829 - accuracy: 0.5250 - val_loss: 1.1006 - val_accuracy: 0.4000\n",
      "Epoch 90/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0822 - accuracy: 0.5250 - val_loss: 1.0999 - val_accuracy: 0.4000\n",
      "Epoch 91/250\n",
      "120/120 [==============================] - 0s 617us/sample - loss: 1.0815 - accuracy: 0.5333 - val_loss: 1.0994 - val_accuracy: 0.4000\n",
      "Epoch 92/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0808 - accuracy: 0.5333 - val_loss: 1.0988 - val_accuracy: 0.4000\n",
      "Epoch 93/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 1.0801 - accuracy: 0.5583 - val_loss: 1.0982 - val_accuracy: 0.4000\n",
      "Epoch 94/250\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 1.0794 - accuracy: 0.5667 - val_loss: 1.0975 - val_accuracy: 0.4000\n",
      "Epoch 95/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0786 - accuracy: 0.5667 - val_loss: 1.0968 - val_accuracy: 0.4000\n",
      "Epoch 96/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 1.0778 - accuracy: 0.5833 - val_loss: 1.0961 - val_accuracy: 0.4000\n",
      "Epoch 97/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0770 - accuracy: 0.5917 - val_loss: 1.0953 - val_accuracy: 0.4333\n",
      "Epoch 98/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0761 - accuracy: 0.5917 - val_loss: 1.0947 - val_accuracy: 0.4667\n",
      "Epoch 99/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 1.0754 - accuracy: 0.6083 - val_loss: 1.0940 - val_accuracy: 0.4667\n",
      "Epoch 100/250\n",
      "120/120 [==============================] - 0s 600us/sample - loss: 1.0745 - accuracy: 0.6333 - val_loss: 1.0932 - val_accuracy: 0.4667\n",
      "Epoch 101/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0736 - accuracy: 0.6333 - val_loss: 1.0924 - val_accuracy: 0.4667\n",
      "Epoch 102/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0726 - accuracy: 0.6333 - val_loss: 1.0916 - val_accuracy: 0.4667\n",
      "Epoch 103/250\n",
      "120/120 [==============================] - 0s 567us/sample - loss: 1.0717 - accuracy: 0.6417 - val_loss: 1.0908 - val_accuracy: 0.4667\n",
      "Epoch 104/250\n",
      "120/120 [==============================] - 0s 617us/sample - loss: 1.0708 - accuracy: 0.6417 - val_loss: 1.0900 - val_accuracy: 0.4667\n",
      "Epoch 105/250\n",
      "120/120 [==============================] - 0s 650us/sample - loss: 1.0697 - accuracy: 0.6500 - val_loss: 1.0892 - val_accuracy: 0.5000\n",
      "Epoch 106/250\n",
      "120/120 [==============================] - 0s 442us/sample - loss: 1.0688 - accuracy: 0.6500 - val_loss: 1.0882 - val_accuracy: 0.5333\n",
      "Epoch 107/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0678 - accuracy: 0.6500 - val_loss: 1.0872 - val_accuracy: 0.5333\n",
      "Epoch 108/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0667 - accuracy: 0.6500 - val_loss: 1.0862 - val_accuracy: 0.5333\n",
      "Epoch 109/250\n",
      "120/120 [==============================] - 0s 558us/sample - loss: 1.0657 - accuracy: 0.6500 - val_loss: 1.0852 - val_accuracy: 0.5333\n",
      "Epoch 110/250\n",
      "120/120 [==============================] - 0s 633us/sample - loss: 1.0646 - accuracy: 0.6500 - val_loss: 1.0842 - val_accuracy: 0.5333\n",
      "Epoch 111/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0634 - accuracy: 0.6500 - val_loss: 1.0832 - val_accuracy: 0.5333\n",
      "Epoch 112/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.0623 - accuracy: 0.6500 - val_loss: 1.0821 - val_accuracy: 0.5333\n",
      "Epoch 113/250\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 1.0612 - accuracy: 0.6500 - val_loss: 1.0809 - val_accuracy: 0.5333\n",
      "Epoch 114/250\n",
      "120/120 [==============================] - 0s 567us/sample - loss: 1.0599 - accuracy: 0.6500 - val_loss: 1.0799 - val_accuracy: 0.5333\n",
      "Epoch 115/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 1.0587 - accuracy: 0.6500 - val_loss: 1.0788 - val_accuracy: 0.5333\n",
      "Epoch 116/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0574 - accuracy: 0.6500 - val_loss: 1.0777 - val_accuracy: 0.5333\n",
      "Epoch 117/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 1.0562 - accuracy: 0.6500 - val_loss: 1.0765 - val_accuracy: 0.5333\n",
      "Epoch 118/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0549 - accuracy: 0.6500 - val_loss: 1.0753 - val_accuracy: 0.5333\n",
      "Epoch 119/250\n",
      "120/120 [==============================] - 0s 558us/sample - loss: 1.0536 - accuracy: 0.6500 - val_loss: 1.0741 - val_accuracy: 0.5333\n",
      "Epoch 120/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0522 - accuracy: 0.6500 - val_loss: 1.0728 - val_accuracy: 0.5333\n",
      "Epoch 121/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0508 - accuracy: 0.6500 - val_loss: 1.0714 - val_accuracy: 0.5333\n",
      "Epoch 122/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0493 - accuracy: 0.6500 - val_loss: 1.0701 - val_accuracy: 0.5333\n",
      "Epoch 123/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.0479 - accuracy: 0.6500 - val_loss: 1.0689 - val_accuracy: 0.5333\n",
      "Epoch 124/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 1.0464 - accuracy: 0.6500 - val_loss: 1.0676 - val_accuracy: 0.5333\n",
      "Epoch 125/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0449 - accuracy: 0.6500 - val_loss: 1.0661 - val_accuracy: 0.5333\n",
      "Epoch 126/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 1.0433 - accuracy: 0.6500 - val_loss: 1.0647 - val_accuracy: 0.5333\n",
      "Epoch 127/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 1.0418 - accuracy: 0.6500 - val_loss: 1.0632 - val_accuracy: 0.5333\n",
      "Epoch 128/250\n",
      "120/120 [==============================] - 0s 667us/sample - loss: 1.0402 - accuracy: 0.6500 - val_loss: 1.0617 - val_accuracy: 0.5333\n",
      "Epoch 129/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0386 - accuracy: 0.6500 - val_loss: 1.0602 - val_accuracy: 0.5333\n",
      "Epoch 130/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 1.0370 - accuracy: 0.6500 - val_loss: 1.0587 - val_accuracy: 0.5333\n",
      "Epoch 131/250\n",
      "120/120 [==============================] - 0s 725us/sample - loss: 1.0353 - accuracy: 0.6500 - val_loss: 1.0571 - val_accuracy: 0.5333\n",
      "Epoch 132/250\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 1.0336 - accuracy: 0.6500 - val_loss: 1.0554 - val_accuracy: 0.5333\n",
      "Epoch 133/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.0318 - accuracy: 0.6500 - val_loss: 1.0538 - val_accuracy: 0.5333\n",
      "Epoch 134/250\n",
      "120/120 [==============================] - 0s 825us/sample - loss: 1.0301 - accuracy: 0.6500 - val_loss: 1.0521 - val_accuracy: 0.5333\n",
      "Epoch 135/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 1.0284 - accuracy: 0.6500 - val_loss: 1.0504 - val_accuracy: 0.5333\n",
      "Epoch 136/250\n",
      "120/120 [==============================] - 0s 500us/sample - loss: 1.0265 - accuracy: 0.6500 - val_loss: 1.0487 - val_accuracy: 0.5333\n",
      "Epoch 137/250\n",
      "120/120 [==============================] - 0s 600us/sample - loss: 1.0247 - accuracy: 0.6500 - val_loss: 1.0470 - val_accuracy: 0.5333\n",
      "Epoch 138/250\n",
      "120/120 [==============================] - 0s 400us/sample - loss: 1.0230 - accuracy: 0.6500 - val_loss: 1.0453 - val_accuracy: 0.5333\n",
      "Epoch 139/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0211 - accuracy: 0.6500 - val_loss: 1.0435 - val_accuracy: 0.5333\n",
      "Epoch 140/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0192 - accuracy: 0.6583 - val_loss: 1.0418 - val_accuracy: 0.5333\n",
      "Epoch 141/250\n",
      "120/120 [==============================] - 0s 708us/sample - loss: 1.0173 - accuracy: 0.6583 - val_loss: 1.0399 - val_accuracy: 0.5333\n",
      "Epoch 142/250\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 1.0153 - accuracy: 0.6583 - val_loss: 1.0381 - val_accuracy: 0.5333\n",
      "Epoch 143/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 1.0135 - accuracy: 0.6583 - val_loss: 1.0362 - val_accuracy: 0.5333\n",
      "Epoch 144/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 1.0114 - accuracy: 0.6583 - val_loss: 1.0343 - val_accuracy: 0.5333\n",
      "Epoch 145/250\n",
      "120/120 [==============================] - 0s 642us/sample - loss: 1.0095 - accuracy: 0.6583 - val_loss: 1.0325 - val_accuracy: 0.5333\n",
      "Epoch 146/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 1.0075 - accuracy: 0.6583 - val_loss: 1.0305 - val_accuracy: 0.5333\n",
      "Epoch 147/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 1.0054 - accuracy: 0.6583 - val_loss: 1.0285 - val_accuracy: 0.5333\n",
      "Epoch 148/250\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.0022 - accuracy: 0.65 - 0s 550us/sample - loss: 1.0034 - accuracy: 0.6583 - val_loss: 1.0265 - val_accuracy: 0.5667\n",
      "Epoch 149/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 1.0012 - accuracy: 0.6583 - val_loss: 1.0245 - val_accuracy: 0.5667\n",
      "Epoch 150/250\n",
      "120/120 [==============================] - 0s 442us/sample - loss: 0.9992 - accuracy: 0.6583 - val_loss: 1.0226 - val_accuracy: 0.5667\n",
      "Epoch 151/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.9971 - accuracy: 0.6583 - val_loss: 1.0205 - val_accuracy: 0.5667\n",
      "Epoch 152/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.9949 - accuracy: 0.6583 - val_loss: 1.0184 - val_accuracy: 0.5667\n",
      "Epoch 153/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.9926 - accuracy: 0.6583 - val_loss: 1.0163 - val_accuracy: 0.5667\n",
      "Epoch 154/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.9906 - accuracy: 0.6583 - val_loss: 1.0142 - val_accuracy: 0.5667\n",
      "Epoch 155/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 0.9883 - accuracy: 0.6583 - val_loss: 1.0120 - val_accuracy: 0.5667\n",
      "Epoch 156/250\n",
      "120/120 [==============================] - 0s 600us/sample - loss: 0.9860 - accuracy: 0.6583 - val_loss: 1.0098 - val_accuracy: 0.5667\n",
      "Epoch 157/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.9837 - accuracy: 0.6583 - val_loss: 1.0077 - val_accuracy: 0.5667\n",
      "Epoch 158/250\n",
      "120/120 [==============================] - 0s 633us/sample - loss: 0.9815 - accuracy: 0.6583 - val_loss: 1.0056 - val_accuracy: 0.5667\n",
      "Epoch 159/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.9791 - accuracy: 0.6583 - val_loss: 1.0034 - val_accuracy: 0.5667\n",
      "Epoch 160/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.9769 - accuracy: 0.6583 - val_loss: 1.0013 - val_accuracy: 0.5667\n",
      "Epoch 161/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9746 - accuracy: 0.6583 - val_loss: 0.9990 - val_accuracy: 0.5667\n",
      "Epoch 162/250\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.9762 - accuracy: 0.62 - 0s 500us/sample - loss: 0.9722 - accuracy: 0.6583 - val_loss: 0.9969 - val_accuracy: 0.5667\n",
      "Epoch 163/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 0.9697 - accuracy: 0.6583 - val_loss: 0.9947 - val_accuracy: 0.5667\n",
      "Epoch 164/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 0.9673 - accuracy: 0.6583 - val_loss: 0.9925 - val_accuracy: 0.5667\n",
      "Epoch 165/250\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 0.9651 - accuracy: 0.6583 - val_loss: 0.9904 - val_accuracy: 0.6000\n",
      "Epoch 166/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.9627 - accuracy: 0.6583 - val_loss: 0.9881 - val_accuracy: 0.6000\n",
      "Epoch 167/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9601 - accuracy: 0.6583 - val_loss: 0.9858 - val_accuracy: 0.6000\n",
      "Epoch 168/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.9577 - accuracy: 0.6583 - val_loss: 0.9835 - val_accuracy: 0.6000\n",
      "Epoch 169/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9552 - accuracy: 0.6583 - val_loss: 0.9811 - val_accuracy: 0.6000\n",
      "Epoch 170/250\n",
      "120/120 [==============================] - 0s 558us/sample - loss: 0.9528 - accuracy: 0.6583 - val_loss: 0.9787 - val_accuracy: 0.6000\n",
      "Epoch 171/250\n",
      "120/120 [==============================] - 0s 642us/sample - loss: 0.9502 - accuracy: 0.6583 - val_loss: 0.9765 - val_accuracy: 0.6000\n",
      "Epoch 172/250\n",
      "120/120 [==============================] - 0s 408us/sample - loss: 0.9478 - accuracy: 0.6583 - val_loss: 0.9741 - val_accuracy: 0.6000\n",
      "Epoch 173/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 0.9453 - accuracy: 0.6583 - val_loss: 0.9718 - val_accuracy: 0.6000\n",
      "Epoch 174/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 0.9428 - accuracy: 0.6583 - val_loss: 0.9694 - val_accuracy: 0.6000\n",
      "Epoch 175/250\n",
      "120/120 [==============================] - 0s 383us/sample - loss: 0.9402 - accuracy: 0.6583 - val_loss: 0.9670 - val_accuracy: 0.6000\n",
      "Epoch 176/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9377 - accuracy: 0.6667 - val_loss: 0.9647 - val_accuracy: 0.6000\n",
      "Epoch 177/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.9351 - accuracy: 0.6667 - val_loss: 0.9623 - val_accuracy: 0.6000\n",
      "Epoch 178/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.9325 - accuracy: 0.6667 - val_loss: 0.9598 - val_accuracy: 0.6000\n",
      "Epoch 179/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9300 - accuracy: 0.6667 - val_loss: 0.9574 - val_accuracy: 0.6000\n",
      "Epoch 180/250\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 0.9274 - accuracy: 0.6667 - val_loss: 0.9549 - val_accuracy: 0.6000\n",
      "Epoch 181/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.9248 - accuracy: 0.6667 - val_loss: 0.9524 - val_accuracy: 0.6000\n",
      "Epoch 182/250\n",
      "120/120 [==============================] - 0s 442us/sample - loss: 0.9222 - accuracy: 0.6667 - val_loss: 0.9500 - val_accuracy: 0.6000\n",
      "Epoch 183/250\n",
      "120/120 [==============================] - 0s 633us/sample - loss: 0.9196 - accuracy: 0.6667 - val_loss: 0.9475 - val_accuracy: 0.6000\n",
      "Epoch 184/250\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.9169 - accuracy: 0.6667 - val_loss: 0.9451 - val_accuracy: 0.6000\n",
      "Epoch 185/250\n",
      "120/120 [==============================] - 0s 633us/sample - loss: 0.9144 - accuracy: 0.6667 - val_loss: 0.9426 - val_accuracy: 0.6000\n",
      "Epoch 186/250\n",
      "120/120 [==============================] - 0s 700us/sample - loss: 0.9119 - accuracy: 0.6667 - val_loss: 0.9401 - val_accuracy: 0.6000\n",
      "Epoch 187/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.9092 - accuracy: 0.6667 - val_loss: 0.9377 - val_accuracy: 0.6000\n",
      "Epoch 188/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.9066 - accuracy: 0.6667 - val_loss: 0.9352 - val_accuracy: 0.6000\n",
      "Epoch 189/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.9041 - accuracy: 0.6667 - val_loss: 0.9327 - val_accuracy: 0.6000\n",
      "Epoch 190/250\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 0.9014 - accuracy: 0.6667 - val_loss: 0.9303 - val_accuracy: 0.6000\n",
      "Epoch 191/250\n",
      "120/120 [==============================] - 0s 408us/sample - loss: 0.8989 - accuracy: 0.6667 - val_loss: 0.9278 - val_accuracy: 0.6000\n",
      "Epoch 192/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.8963 - accuracy: 0.6667 - val_loss: 0.9254 - val_accuracy: 0.6000\n",
      "Epoch 193/250\n",
      "120/120 [==============================] - 0s 725us/sample - loss: 0.8938 - accuracy: 0.6667 - val_loss: 0.9229 - val_accuracy: 0.6000\n",
      "Epoch 194/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.8911 - accuracy: 0.6667 - val_loss: 0.9206 - val_accuracy: 0.6000\n",
      "Epoch 195/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 0.8886 - accuracy: 0.6667 - val_loss: 0.9182 - val_accuracy: 0.6000\n",
      "Epoch 196/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.8860 - accuracy: 0.6667 - val_loss: 0.9157 - val_accuracy: 0.6000\n",
      "Epoch 197/250\n",
      "120/120 [==============================] - 0s 442us/sample - loss: 0.8835 - accuracy: 0.6667 - val_loss: 0.9132 - val_accuracy: 0.6000\n",
      "Epoch 198/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 0.8808 - accuracy: 0.6667 - val_loss: 0.9108 - val_accuracy: 0.6000\n",
      "Epoch 199/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.8782 - accuracy: 0.6667 - val_loss: 0.9084 - val_accuracy: 0.6000\n",
      "Epoch 200/250\n",
      "120/120 [==============================] - 0s 733us/sample - loss: 0.8758 - accuracy: 0.6667 - val_loss: 0.9060 - val_accuracy: 0.6000\n",
      "Epoch 201/250\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 0.8732 - accuracy: 0.6667 - val_loss: 0.9035 - val_accuracy: 0.6000\n",
      "Epoch 202/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.8707 - accuracy: 0.6667 - val_loss: 0.9011 - val_accuracy: 0.6000\n",
      "Epoch 203/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.8681 - accuracy: 0.6667 - val_loss: 0.8987 - val_accuracy: 0.6000\n",
      "Epoch 204/250\n",
      "120/120 [==============================] - 0s 417us/sample - loss: 0.8656 - accuracy: 0.6750 - val_loss: 0.8963 - val_accuracy: 0.6000\n",
      "Epoch 205/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.8630 - accuracy: 0.6750 - val_loss: 0.8939 - val_accuracy: 0.6000\n",
      "Epoch 206/250\n",
      "120/120 [==============================] - 0s 417us/sample - loss: 0.8606 - accuracy: 0.6750 - val_loss: 0.8916 - val_accuracy: 0.6000\n",
      "Epoch 207/250\n",
      "120/120 [==============================] - 0s 617us/sample - loss: 0.8580 - accuracy: 0.6750 - val_loss: 0.8892 - val_accuracy: 0.6000\n",
      "Epoch 208/250\n",
      "120/120 [==============================] - 0s 500us/sample - loss: 0.8556 - accuracy: 0.6750 - val_loss: 0.8868 - val_accuracy: 0.6000\n",
      "Epoch 209/250\n",
      "120/120 [==============================] - 0s 392us/sample - loss: 0.8531 - accuracy: 0.6750 - val_loss: 0.8844 - val_accuracy: 0.6000\n",
      "Epoch 210/250\n",
      "120/120 [==============================] - 0s 417us/sample - loss: 0.8505 - accuracy: 0.6750 - val_loss: 0.8820 - val_accuracy: 0.6000\n",
      "Epoch 211/250\n",
      "120/120 [==============================] - 0s 717us/sample - loss: 0.8481 - accuracy: 0.6750 - val_loss: 0.8797 - val_accuracy: 0.6000\n",
      "Epoch 212/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.8455 - accuracy: 0.6750 - val_loss: 0.8773 - val_accuracy: 0.6000\n",
      "Epoch 213/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.8431 - accuracy: 0.6750 - val_loss: 0.8749 - val_accuracy: 0.6000\n",
      "Epoch 214/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 0.8407 - accuracy: 0.6750 - val_loss: 0.8725 - val_accuracy: 0.6000\n",
      "Epoch 215/250\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.8382 - accuracy: 0.6750 - val_loss: 0.8701 - val_accuracy: 0.6000\n",
      "Epoch 216/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 0.8357 - accuracy: 0.6750 - val_loss: 0.8678 - val_accuracy: 0.6000\n",
      "Epoch 217/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 0.8333 - accuracy: 0.6750 - val_loss: 0.8655 - val_accuracy: 0.6000\n",
      "Epoch 218/250\n",
      "120/120 [==============================] - 0s 625us/sample - loss: 0.8308 - accuracy: 0.6750 - val_loss: 0.8631 - val_accuracy: 0.6000\n",
      "Epoch 219/250\n",
      "120/120 [==============================] - 0s 575us/sample - loss: 0.8284 - accuracy: 0.6750 - val_loss: 0.8608 - val_accuracy: 0.6000\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 550us/sample - loss: 0.8259 - accuracy: 0.6750 - val_loss: 0.8586 - val_accuracy: 0.6000\n",
      "Epoch 221/250\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 0.8235 - accuracy: 0.6750 - val_loss: 0.8563 - val_accuracy: 0.6000\n",
      "Epoch 222/250\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8456 - accuracy: 0.65 - 0s 583us/sample - loss: 0.8210 - accuracy: 0.6750 - val_loss: 0.8540 - val_accuracy: 0.6000\n",
      "Epoch 223/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 0.8187 - accuracy: 0.6750 - val_loss: 0.8516 - val_accuracy: 0.6000\n",
      "Epoch 224/250\n",
      "120/120 [==============================] - 0s 850us/sample - loss: 0.8162 - accuracy: 0.6750 - val_loss: 0.8493 - val_accuracy: 0.6000\n",
      "Epoch 225/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.8138 - accuracy: 0.6750 - val_loss: 0.8470 - val_accuracy: 0.6000\n",
      "Epoch 226/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.8114 - accuracy: 0.6750 - val_loss: 0.8447 - val_accuracy: 0.6000\n",
      "Epoch 227/250\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 0.8090 - accuracy: 0.6750 - val_loss: 0.8425 - val_accuracy: 0.6000\n",
      "Epoch 228/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.8067 - accuracy: 0.6750 - val_loss: 0.8402 - val_accuracy: 0.6000\n",
      "Epoch 229/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 0.8043 - accuracy: 0.6750 - val_loss: 0.8380 - val_accuracy: 0.6000\n",
      "Epoch 230/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 0.8020 - accuracy: 0.6750 - val_loss: 0.8359 - val_accuracy: 0.6000\n",
      "Epoch 231/250\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.7997 - accuracy: 0.6750 - val_loss: 0.8336 - val_accuracy: 0.6000\n",
      "Epoch 232/250\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 0.7973 - accuracy: 0.6750 - val_loss: 0.8314 - val_accuracy: 0.6000\n",
      "Epoch 233/250\n",
      "120/120 [==============================] - 0s 567us/sample - loss: 0.7951 - accuracy: 0.6750 - val_loss: 0.8292 - val_accuracy: 0.6000\n",
      "Epoch 234/250\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.7927 - accuracy: 0.6750 - val_loss: 0.8272 - val_accuracy: 0.6000\n",
      "Epoch 235/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 0.7905 - accuracy: 0.6833 - val_loss: 0.8250 - val_accuracy: 0.6000\n",
      "Epoch 236/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.7882 - accuracy: 0.6833 - val_loss: 0.8228 - val_accuracy: 0.6000\n",
      "Epoch 237/250\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 0.7860 - accuracy: 0.6833 - val_loss: 0.8207 - val_accuracy: 0.6000\n",
      "Epoch 238/250\n",
      "120/120 [==============================] - 0s 583us/sample - loss: 0.7838 - accuracy: 0.6833 - val_loss: 0.8186 - val_accuracy: 0.6000\n",
      "Epoch 239/250\n",
      "120/120 [==============================] - 0s 592us/sample - loss: 0.7816 - accuracy: 0.6833 - val_loss: 0.8164 - val_accuracy: 0.6000\n",
      "Epoch 240/250\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.7794 - accuracy: 0.6833 - val_loss: 0.8144 - val_accuracy: 0.6000\n",
      "Epoch 241/250\n",
      "120/120 [==============================] - 0s 450us/sample - loss: 0.7772 - accuracy: 0.6833 - val_loss: 0.8124 - val_accuracy: 0.6000\n",
      "Epoch 242/250\n",
      "120/120 [==============================] - 0s 500us/sample - loss: 0.7750 - accuracy: 0.6833 - val_loss: 0.8104 - val_accuracy: 0.6000\n",
      "Epoch 243/250\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 0.7728 - accuracy: 0.6833 - val_loss: 0.8084 - val_accuracy: 0.6000\n",
      "Epoch 244/250\n",
      "120/120 [==============================] - 0s 708us/sample - loss: 0.7707 - accuracy: 0.6833 - val_loss: 0.8063 - val_accuracy: 0.6000\n",
      "Epoch 245/250\n",
      "120/120 [==============================] - 0s 550us/sample - loss: 0.7685 - accuracy: 0.6833 - val_loss: 0.8043 - val_accuracy: 0.6000\n",
      "Epoch 246/250\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.7664 - accuracy: 0.6833 - val_loss: 0.8023 - val_accuracy: 0.6000\n",
      "Epoch 247/250\n",
      "120/120 [==============================] - 0s 683us/sample - loss: 0.7643 - accuracy: 0.6833 - val_loss: 0.8004 - val_accuracy: 0.6000\n",
      "Epoch 248/250\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 0.7622 - accuracy: 0.6833 - val_loss: 0.7984 - val_accuracy: 0.6000\n",
      "Epoch 249/250\n",
      "120/120 [==============================] - 0s 658us/sample - loss: 0.7601 - accuracy: 0.6833 - val_loss: 0.7964 - val_accuracy: 0.6000\n",
      "Epoch 250/250\n",
      "120/120 [==============================] - 0s 883us/sample - loss: 0.7580 - accuracy: 0.6833 - val_loss: 0.7944 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1757a708>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaler_x_train,y=y_train,epochs=250,\n",
    "        validation_data=(scaler_x_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.239089</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.199161</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.232266</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.194475</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.225065</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.189983</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.218076</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.185717</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.212761</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.181768</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.766384</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.802312</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.764276</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.800376</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.762170</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.798365</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.760082</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.796398</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.758002</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.794418</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    1.239089  0.291667  1.199161      0.300000\n",
       "1    1.232266  0.291667  1.194475      0.300000\n",
       "2    1.225065  0.291667  1.189983      0.300000\n",
       "3    1.218076  0.291667  1.185717      0.300000\n",
       "4    1.212761  0.291667  1.181768      0.266667\n",
       "..        ...       ...       ...           ...\n",
       "245  0.766384  0.683333  0.802312      0.600000\n",
       "246  0.764276  0.683333  0.800376      0.600000\n",
       "247  0.762170  0.683333  0.798365      0.600000\n",
       "248  0.760082  0.683333  0.796398      0.600000\n",
       "249  0.758002  0.683333  0.794418      0.600000\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(model.history.history)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1914ce08>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[[\"loss\",\"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.794417679309845, 0.6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaler_x_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feeding all data and creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "model2.add(Dense(units=4,activation=\"relu\"))\n",
    "model2.add(Dense(units=3,activation=\"softmax\"))\n",
    "\n",
    "model2.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/250\n",
      "150/150 [==============================] - 1s 5ms/sample - loss: 1.0972 - accuracy: 0.1733\n",
      "Epoch 2/250\n",
      "150/150 [==============================] - 0s 320us/sample - loss: 1.0960 - accuracy: 0.3333\n",
      "Epoch 3/250\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 1.0951 - accuracy: 0.3333\n",
      "Epoch 4/250\n",
      "150/150 [==============================] - 0s 353us/sample - loss: 1.0941 - accuracy: 0.3267\n",
      "Epoch 5/250\n",
      "150/150 [==============================] - 0s 307us/sample - loss: 1.0933 - accuracy: 0.3267\n",
      "Epoch 6/250\n",
      "150/150 [==============================] - 0s 300us/sample - loss: 1.0925 - accuracy: 0.3267\n",
      "Epoch 7/250\n",
      "150/150 [==============================] - 0s 433us/sample - loss: 1.0917 - accuracy: 0.3267\n",
      "Epoch 8/250\n",
      "150/150 [==============================] - 0s 393us/sample - loss: 1.0907 - accuracy: 0.3333\n",
      "Epoch 9/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 1.0897 - accuracy: 0.3333\n",
      "Epoch 10/250\n",
      "150/150 [==============================] - 0s 287us/sample - loss: 1.0885 - accuracy: 0.3200\n",
      "Epoch 11/250\n",
      "150/150 [==============================] - 0s 387us/sample - loss: 1.0872 - accuracy: 0.3200\n",
      "Epoch 12/250\n",
      "150/150 [==============================] - 0s 380us/sample - loss: 1.0859 - accuracy: 0.3200\n",
      "Epoch 13/250\n",
      "150/150 [==============================] - 0s 400us/sample - loss: 1.0844 - accuracy: 0.3200\n",
      "Epoch 14/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 1.0841 - accuracy: 0.34 - 0s 333us/sample - loss: 1.0829 - accuracy: 0.3200\n",
      "Epoch 15/250\n",
      "150/150 [==============================] - 0s 827us/sample - loss: 1.0812 - accuracy: 0.3200\n",
      "Epoch 16/250\n",
      "150/150 [==============================] - 0s 587us/sample - loss: 1.0793 - accuracy: 0.3200\n",
      "Epoch 17/250\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 1.0773 - accuracy: 0.3200\n",
      "Epoch 18/250\n",
      "150/150 [==============================] - 0s 760us/sample - loss: 1.0751 - accuracy: 0.3200\n",
      "Epoch 19/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 1.0729 - accuracy: 0.3200\n",
      "Epoch 20/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 1.0704 - accuracy: 0.3200\n",
      "Epoch 21/250\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 1.0679 - accuracy: 0.5600\n",
      "Epoch 22/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 1.0651 - accuracy: 0.6533\n",
      "Epoch 23/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 1.0621 - accuracy: 0.6533\n",
      "Epoch 24/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 1.0590 - accuracy: 0.6533\n",
      "Epoch 25/250\n",
      "150/150 [==============================] - 0s 260us/sample - loss: 1.0561 - accuracy: 0.6533\n",
      "Epoch 26/250\n",
      "150/150 [==============================] - 0s 427us/sample - loss: 1.0528 - accuracy: 0.6533\n",
      "Epoch 27/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 1.0496 - accuracy: 0.6533\n",
      "Epoch 28/250\n",
      "150/150 [==============================] - 0s 287us/sample - loss: 1.0463 - accuracy: 0.6533\n",
      "Epoch 29/250\n",
      "150/150 [==============================] - 0s 360us/sample - loss: 1.0429 - accuracy: 0.6533\n",
      "Epoch 30/250\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 1.0390 - accuracy: 0.6533\n",
      "Epoch 31/250\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 1.0354 - accuracy: 0.6533\n",
      "Epoch 32/250\n",
      "150/150 [==============================] - 0s 740us/sample - loss: 1.0316 - accuracy: 0.6533\n",
      "Epoch 33/250\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 1.0273 - accuracy: 0.6533\n",
      "Epoch 34/250\n",
      "150/150 [==============================] - 0s 760us/sample - loss: 1.0231 - accuracy: 0.6533\n",
      "Epoch 35/250\n",
      "150/150 [==============================] - 0s 513us/sample - loss: 1.0190 - accuracy: 0.6533\n",
      "Epoch 36/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 1.0147 - accuracy: 0.6533\n",
      "Epoch 37/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 1.0338 - accuracy: 0.56 - 0s 240us/sample - loss: 1.0099 - accuracy: 0.6533\n",
      "Epoch 38/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 1.0052 - accuracy: 0.6533\n",
      "Epoch 39/250\n",
      "150/150 [==============================] - 0s 480us/sample - loss: 1.0003 - accuracy: 0.6533\n",
      "Epoch 40/250\n",
      "150/150 [==============================] - 0s 407us/sample - loss: 0.9957 - accuracy: 0.6533\n",
      "Epoch 41/250\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 0.9904 - accuracy: 0.6533\n",
      "Epoch 42/250\n",
      "150/150 [==============================] - 0s 367us/sample - loss: 0.9854 - accuracy: 0.6533\n",
      "Epoch 43/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.9802 - accuracy: 0.6533\n",
      "Epoch 44/250\n",
      "150/150 [==============================] - 0s 1ms/sample - loss: 0.9746 - accuracy: 0.6600\n",
      "Epoch 45/250\n",
      "150/150 [==============================] - 0s 940us/sample - loss: 0.9695 - accuracy: 0.6600\n",
      "Epoch 46/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.9641 - accuracy: 0.6600\n",
      "Epoch 47/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.9583 - accuracy: 0.6600\n",
      "Epoch 48/250\n",
      "150/150 [==============================] - 0s 440us/sample - loss: 0.9528 - accuracy: 0.6600\n",
      "Epoch 49/250\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 0.9471 - accuracy: 0.6600\n",
      "Epoch 50/250\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 0.9413 - accuracy: 0.6600\n",
      "Epoch 51/250\n",
      "150/150 [==============================] - 0s 967us/sample - loss: 0.9356 - accuracy: 0.6600\n",
      "Epoch 52/250\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 0.9296 - accuracy: 0.6600\n",
      "Epoch 53/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.9238 - accuracy: 0.6600\n",
      "Epoch 54/250\n",
      "150/150 [==============================] - 0s 720us/sample - loss: 0.9180 - accuracy: 0.6600\n",
      "Epoch 55/250\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.9123 - accuracy: 0.6600\n",
      "Epoch 56/250\n",
      "150/150 [==============================] - 0s 367us/sample - loss: 0.9060 - accuracy: 0.6600\n",
      "Epoch 57/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.65 - 0s 153us/sample - loss: 0.9004 - accuracy: 0.6600\n",
      "Epoch 58/250\n",
      "150/150 [==============================] - 0s 413us/sample - loss: 0.8944 - accuracy: 0.6600\n",
      "Epoch 59/250\n",
      "150/150 [==============================] - 0s 360us/sample - loss: 0.8885 - accuracy: 0.6667\n",
      "Epoch 60/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8670 - accuracy: 0.65 - 0s 267us/sample - loss: 0.8826 - accuracy: 0.6667\n",
      "Epoch 61/250\n",
      "150/150 [==============================] - 0s 353us/sample - loss: 0.8767 - accuracy: 0.6667\n",
      "Epoch 62/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.8707 - accuracy: 0.6667\n",
      "Epoch 63/250\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8652 - accuracy: 0.6667\n",
      "Epoch 64/250\n",
      "150/150 [==============================] - 0s 287us/sample - loss: 0.8596 - accuracy: 0.6667\n",
      "Epoch 65/250\n",
      "150/150 [==============================] - 0s 253us/sample - loss: 0.8535 - accuracy: 0.6667\n",
      "Epoch 66/250\n",
      "150/150 [==============================] - 0s 620us/sample - loss: 0.8480 - accuracy: 0.6667\n",
      "Epoch 67/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8365 - accuracy: 0.65 - 0s 473us/sample - loss: 0.8424 - accuracy: 0.6667\n",
      "Epoch 68/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.8369 - accuracy: 0.6667\n",
      "Epoch 69/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.8315 - accuracy: 0.6667\n",
      "Epoch 70/250\n",
      "150/150 [==============================] - 0s 407us/sample - loss: 0.8260 - accuracy: 0.6667\n",
      "Epoch 71/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.8208 - accuracy: 0.6667\n",
      "Epoch 72/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.8156 - accuracy: 0.6667\n",
      "Epoch 73/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8586 - accuracy: 0.75 - 0s 267us/sample - loss: 0.8103 - accuracy: 0.6667\n",
      "Epoch 74/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.8053 - accuracy: 0.6667\n",
      "Epoch 75/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.8003 - accuracy: 0.6667\n",
      "Epoch 76/250\n",
      "150/150 [==============================] - 0s 367us/sample - loss: 0.7953 - accuracy: 0.6667\n",
      "Epoch 77/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7346 - accuracy: 0.78 - 0s 200us/sample - loss: 0.7906 - accuracy: 0.6667\n",
      "Epoch 78/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.7857 - accuracy: 0.6667\n",
      "Epoch 79/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.7811 - accuracy: 0.6667\n",
      "Epoch 80/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.7765 - accuracy: 0.6667\n",
      "Epoch 81/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.7719 - accuracy: 0.6667\n",
      "Epoch 82/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7792 - accuracy: 0.65 - 0s 113us/sample - loss: 0.7675 - accuracy: 0.6667\n",
      "Epoch 83/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.7632 - accuracy: 0.6667\n",
      "Epoch 84/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.7590 - accuracy: 0.6667\n",
      "Epoch 85/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.7547 - accuracy: 0.6667\n",
      "Epoch 86/250\n",
      "150/150 [==============================] - 0s 260us/sample - loss: 0.7507 - accuracy: 0.6667\n",
      "Epoch 87/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.7466 - accuracy: 0.6667\n",
      "Epoch 88/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.7428 - accuracy: 0.6667\n",
      "Epoch 89/250\n",
      "150/150 [==============================] - 0s 427us/sample - loss: 0.7388 - accuracy: 0.6667\n",
      "Epoch 90/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.7351 - accuracy: 0.6667\n",
      "Epoch 91/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.7313 - accuracy: 0.6667\n",
      "Epoch 92/250\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.7278 - accuracy: 0.6667\n",
      "Epoch 93/250\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.7242 - accuracy: 0.6667\n",
      "Epoch 94/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.7208 - accuracy: 0.6667\n",
      "Epoch 95/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.7174 - accuracy: 0.6667\n",
      "Epoch 96/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.7141 - accuracy: 0.6667\n",
      "Epoch 97/250\n",
      "150/150 [==============================] - 0s 300us/sample - loss: 0.7108 - accuracy: 0.6667\n",
      "Epoch 98/250\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.7077 - accuracy: 0.6667\n",
      "Epoch 99/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.7045 - accuracy: 0.6667\n",
      "Epoch 100/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.7014 - accuracy: 0.6667\n",
      "Epoch 101/250\n",
      "150/150 [==============================] - 0s 473us/sample - loss: 0.6985 - accuracy: 0.6667\n",
      "Epoch 102/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.6955 - accuracy: 0.6667\n",
      "Epoch 103/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.6926 - accuracy: 0.6667\n",
      "Epoch 104/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.6898 - accuracy: 0.6667\n",
      "Epoch 105/250\n",
      "150/150 [==============================] - 0s 160us/sample - loss: 0.6871 - accuracy: 0.6667\n",
      "Epoch 106/250\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.6843 - accuracy: 0.6667\n",
      "Epoch 107/250\n",
      "150/150 [==============================] - 0s 287us/sample - loss: 0.6817 - accuracy: 0.6667\n",
      "Epoch 108/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.6791 - accuracy: 0.6667\n",
      "Epoch 109/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.6766 - accuracy: 0.6667\n",
      "Epoch 110/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.6742 - accuracy: 0.6667\n",
      "Epoch 111/250\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.6717 - accuracy: 0.6667\n",
      "Epoch 112/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.6693 - accuracy: 0.6667\n",
      "Epoch 113/250\n",
      "150/150 [==============================] - 0s 367us/sample - loss: 0.6670 - accuracy: 0.6667\n",
      "Epoch 114/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.6647 - accuracy: 0.6667\n",
      "Epoch 115/250\n",
      "150/150 [==============================] - 0s 360us/sample - loss: 0.6624 - accuracy: 0.6667\n",
      "Epoch 116/250\n",
      "150/150 [==============================] - 0s 813us/sample - loss: 0.6602 - accuracy: 0.6667\n",
      "Epoch 117/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.6580 - accuracy: 0.6667\n",
      "Epoch 118/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.6559 - accuracy: 0.6667\n",
      "Epoch 119/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.6538 - accuracy: 0.6667\n",
      "Epoch 120/250\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 0.6517 - accuracy: 0.6667\n",
      "Epoch 121/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.6497 - accuracy: 0.6667\n",
      "Epoch 122/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.6477 - accuracy: 0.6667\n",
      "Epoch 123/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.6458 - accuracy: 0.6667\n",
      "Epoch 124/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.6439 - accuracy: 0.6667\n",
      "Epoch 125/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.6420 - accuracy: 0.6667\n",
      "Epoch 126/250\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.6402 - accuracy: 0.6667\n",
      "Epoch 127/250\n",
      "150/150 [==============================] - 0s 280us/sample - loss: 0.6384 - accuracy: 0.6667\n",
      "Epoch 128/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.6366 - accuracy: 0.6667\n",
      "Epoch 129/250\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6349 - accuracy: 0.6667\n",
      "Epoch 130/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.6332 - accuracy: 0.6667\n",
      "Epoch 131/250\n",
      "150/150 [==============================] - 0s 347us/sample - loss: 0.6315 - accuracy: 0.6667\n",
      "Epoch 132/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.6298 - accuracy: 0.6667\n",
      "Epoch 133/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.6282 - accuracy: 0.6667\n",
      "Epoch 134/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.6266 - accuracy: 0.6667\n",
      "Epoch 135/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.6250 - accuracy: 0.6667\n",
      "Epoch 136/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.6235 - accuracy: 0.6667\n",
      "Epoch 137/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.6219 - accuracy: 0.6667\n",
      "Epoch 138/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.6204 - accuracy: 0.6667\n",
      "Epoch 139/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.6189 - accuracy: 0.6667\n",
      "Epoch 140/250\n",
      "150/150 [==============================] - 0s 260us/sample - loss: 0.6175 - accuracy: 0.6667\n",
      "Epoch 141/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.6161 - accuracy: 0.6667\n",
      "Epoch 142/250\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.6147 - accuracy: 0.6667\n",
      "Epoch 143/250\n",
      "150/150 [==============================] - 0s 353us/sample - loss: 0.6133 - accuracy: 0.6667\n",
      "Epoch 144/250\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.6119 - accuracy: 0.6667\n",
      "Epoch 145/250\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6106 - accuracy: 0.6667\n",
      "Epoch 146/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.6093 - accuracy: 0.6667\n",
      "Epoch 147/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.6080 - accuracy: 0.6667\n",
      "Epoch 148/250\n",
      "150/150 [==============================] - 0s 333us/sample - loss: 0.6067 - accuracy: 0.6667\n",
      "Epoch 149/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.6054 - accuracy: 0.6667\n",
      "Epoch 150/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.71 - 0s 313us/sample - loss: 0.6042 - accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.6029 - accuracy: 0.6667\n",
      "Epoch 152/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.6017 - accuracy: 0.6667\n",
      "Epoch 153/250\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.65 - 0s 207us/sample - loss: 0.6005 - accuracy: 0.6667\n",
      "Epoch 154/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.5994 - accuracy: 0.6667\n",
      "Epoch 155/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.5982 - accuracy: 0.6667\n",
      "Epoch 156/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.5971 - accuracy: 0.6667\n",
      "Epoch 157/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.5959 - accuracy: 0.6667\n",
      "Epoch 158/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.5948 - accuracy: 0.6667\n",
      "Epoch 159/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5937 - accuracy: 0.6667\n",
      "Epoch 160/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.5927 - accuracy: 0.6667\n",
      "Epoch 161/250\n",
      "150/150 [==============================] - 0s 320us/sample - loss: 0.5916 - accuracy: 0.6667\n",
      "Epoch 162/250\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.5905 - accuracy: 0.6667\n",
      "Epoch 163/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5895 - accuracy: 0.6667\n",
      "Epoch 164/250\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5885 - accuracy: 0.6667\n",
      "Epoch 165/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.5875 - accuracy: 0.6667\n",
      "Epoch 166/250\n",
      "150/150 [==============================] - 0s 260us/sample - loss: 0.5865 - accuracy: 0.6667\n",
      "Epoch 167/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5855 - accuracy: 0.6667\n",
      "Epoch 168/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.5845 - accuracy: 0.6667\n",
      "Epoch 169/250\n",
      "150/150 [==============================] - 0s 280us/sample - loss: 0.5836 - accuracy: 0.6667\n",
      "Epoch 170/250\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.5827 - accuracy: 0.6667\n",
      "Epoch 171/250\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.5817 - accuracy: 0.6667\n",
      "Epoch 172/250\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.5808 - accuracy: 0.6667\n",
      "Epoch 173/250\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.5799 - accuracy: 0.6667\n",
      "Epoch 174/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.5790 - accuracy: 0.6667\n",
      "Epoch 175/250\n",
      "150/150 [==============================] - 0s 407us/sample - loss: 0.5782 - accuracy: 0.6667\n",
      "Epoch 176/250\n",
      "150/150 [==============================] - 0s 280us/sample - loss: 0.5773 - accuracy: 0.6667\n",
      "Epoch 177/250\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 0.5765 - accuracy: 0.6667\n",
      "Epoch 178/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5756 - accuracy: 0.6667\n",
      "Epoch 179/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.5748 - accuracy: 0.6667\n",
      "Epoch 180/250\n",
      "150/150 [==============================] - 0s 260us/sample - loss: 0.5740 - accuracy: 0.6667\n",
      "Epoch 181/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.5732 - accuracy: 0.6667\n",
      "Epoch 182/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.5724 - accuracy: 0.6667\n",
      "Epoch 183/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5716 - accuracy: 0.6667\n",
      "Epoch 184/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.5708 - accuracy: 0.6667\n",
      "Epoch 185/250\n",
      "150/150 [==============================] - 0s 380us/sample - loss: 0.5700 - accuracy: 0.6667\n",
      "Epoch 186/250\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5693 - accuracy: 0.6667\n",
      "Epoch 187/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.5685 - accuracy: 0.6667\n",
      "Epoch 188/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.5678 - accuracy: 0.6667\n",
      "Epoch 189/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.5671 - accuracy: 0.6667\n",
      "Epoch 190/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.5663 - accuracy: 0.6667\n",
      "Epoch 191/250\n",
      "150/150 [==============================] - 0s 420us/sample - loss: 0.5657 - accuracy: 0.6667\n",
      "Epoch 192/250\n",
      "150/150 [==============================] - 0s 280us/sample - loss: 0.5650 - accuracy: 0.6667\n",
      "Epoch 193/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.5642 - accuracy: 0.6667\n",
      "Epoch 194/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.5636 - accuracy: 0.6667\n",
      "Epoch 195/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.5629 - accuracy: 0.6667\n",
      "Epoch 196/250\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.5622 - accuracy: 0.6667\n",
      "Epoch 197/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.5615 - accuracy: 0.6667\n",
      "Epoch 198/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.5609 - accuracy: 0.6667\n",
      "Epoch 199/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.5602 - accuracy: 0.6667\n",
      "Epoch 200/250\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.5596 - accuracy: 0.6667\n",
      "Epoch 201/250\n",
      "150/150 [==============================] - 0s 160us/sample - loss: 0.5590 - accuracy: 0.6667\n",
      "Epoch 202/250\n",
      "150/150 [==============================] - 0s 207us/sample - loss: 0.5583 - accuracy: 0.6667\n",
      "Epoch 203/250\n",
      "150/150 [==============================] - 0s 380us/sample - loss: 0.5577 - accuracy: 0.6667\n",
      "Epoch 204/250\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.5571 - accuracy: 0.6667\n",
      "Epoch 205/250\n",
      "150/150 [==============================] - 0s 287us/sample - loss: 0.5565 - accuracy: 0.6667\n",
      "Epoch 206/250\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 0.5559 - accuracy: 0.6667\n",
      "Epoch 207/250\n",
      "150/150 [==============================] - 0s 700us/sample - loss: 0.5553 - accuracy: 0.6667\n",
      "Epoch 208/250\n",
      "150/150 [==============================] - 0s 493us/sample - loss: 0.5547 - accuracy: 0.6667\n",
      "Epoch 209/250\n",
      "150/150 [==============================] - 0s 533us/sample - loss: 0.5541 - accuracy: 0.6667\n",
      "Epoch 210/250\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 0.5536 - accuracy: 0.6667\n",
      "Epoch 211/250\n",
      "150/150 [==============================] - 0s 387us/sample - loss: 0.5530 - accuracy: 0.6667\n",
      "Epoch 212/250\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.5524 - accuracy: 0.6667\n",
      "Epoch 213/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.5518 - accuracy: 0.6667\n",
      "Epoch 214/250\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.5513 - accuracy: 0.6667\n",
      "Epoch 215/250\n",
      "150/150 [==============================] - 0s 300us/sample - loss: 0.5507 - accuracy: 0.6667\n",
      "Epoch 216/250\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 0.5502 - accuracy: 0.6667\n",
      "Epoch 217/250\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.5497 - accuracy: 0.6600\n",
      "Epoch 218/250\n",
      "150/150 [==============================] - 0s 300us/sample - loss: 0.5491 - accuracy: 0.6600\n",
      "Epoch 219/250\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.5486 - accuracy: 0.6600\n",
      "Epoch 220/250\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.5481 - accuracy: 0.6600\n",
      "Epoch 221/250\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.5476 - accuracy: 0.6600\n",
      "Epoch 222/250\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.5471 - accuracy: 0.6600\n",
      "Epoch 223/250\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.5466 - accuracy: 0.6600\n",
      "Epoch 224/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.5461 - accuracy: 0.6533\n",
      "Epoch 225/250\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.5456 - accuracy: 0.6533\n",
      "Epoch 226/250\n",
      "150/150 [==============================] - 0s 307us/sample - loss: 0.5451 - accuracy: 0.6533\n",
      "Epoch 227/250\n",
      "150/150 [==============================] - 0s 347us/sample - loss: 0.5446 - accuracy: 0.6533\n",
      "Epoch 228/250\n",
      "150/150 [==============================] - 0s 420us/sample - loss: 0.5441 - accuracy: 0.6533\n",
      "Epoch 229/250\n",
      "150/150 [==============================] - 0s 487us/sample - loss: 0.5436 - accuracy: 0.6533\n",
      "Epoch 230/250\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.5432 - accuracy: 0.6533\n",
      "Epoch 231/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.5427 - accuracy: 0.6533\n",
      "Epoch 232/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.5423 - accuracy: 0.6533\n",
      "Epoch 233/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.5418 - accuracy: 0.6533\n",
      "Epoch 234/250\n",
      "150/150 [==============================] - 0s 367us/sample - loss: 0.5413 - accuracy: 0.6533\n",
      "Epoch 235/250\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.5409 - accuracy: 0.6533\n",
      "Epoch 236/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.5405 - accuracy: 0.6533\n",
      "Epoch 237/250\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.5400 - accuracy: 0.6533\n",
      "Epoch 238/250\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.5396 - accuracy: 0.6533\n",
      "Epoch 239/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.5391 - accuracy: 0.6533\n",
      "Epoch 240/250\n",
      "150/150 [==============================] - 0s 267us/sample - loss: 0.5387 - accuracy: 0.6533\n",
      "Epoch 241/250\n",
      "150/150 [==============================] - 0s 247us/sample - loss: 0.5383 - accuracy: 0.6533\n",
      "Epoch 242/250\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 0.5379 - accuracy: 0.6533\n",
      "Epoch 243/250\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.5375 - accuracy: 0.6533\n",
      "Epoch 244/250\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5370 - accuracy: 0.6533\n",
      "Epoch 245/250\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.5366 - accuracy: 0.6533\n",
      "Epoch 246/250\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.5362 - accuracy: 0.6533\n",
      "Epoch 247/250\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.5358 - accuracy: 0.6533\n",
      "Epoch 248/250\n",
      "150/150 [==============================] - 0s 360us/sample - loss: 0.5354 - accuracy: 0.6533\n",
      "Epoch 249/250\n",
      "150/150 [==============================] - 0s 333us/sample - loss: 0.5350 - accuracy: 0.6533\n",
      "Epoch 250/250\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 0.5346 - accuracy: 0.6533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19420108>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(scaled_x,y,epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving our model and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict_classes([[0.22222222, 0.625     , 0.06779661, 0.04166667]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,\"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model=load_model(r\"C:\\Users\\etisalat\\Deep learning - Sep\\final_model.h5\")\n",
    "iris_scaler=joblib.load(r\"C:\\Users\\etisalat\\Deep learning - Sep\\scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=[[5.1,3.5,1.4,0.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_input=iris_scaler.fit_transform(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_model.predict_classes(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input={\"sepal_length\":5.1,\n",
    "            \"sepal_width\":3.5,\n",
    "            \"petal_length\":1.4,\n",
    "            \"petal_width\":0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final code for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model=load_model(r\"C:\\Users\\etisalat\\Deep learning - Sep\\final_model.h5\")\n",
    "iris_scaler=joblib.load(r\"C:\\Users\\etisalat\\Deep learning - Sep\\scaler.pkl\")\n",
    "\n",
    "def iris_pred(model,scaler,input):\n",
    "    \n",
    "    a=input[\"sepal_length\"]\n",
    "    b=input[\"sepal_width\"]\n",
    "    c=input[\"petal_length\"]\n",
    "    d=input[\"petal_width\"]\n",
    "    \n",
    "    \n",
    "    cor_input=[[a,b,c,d]]\n",
    "    \n",
    "    classes=np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    scaled_input=scaler.fit_transform(cor_input)\n",
    "    pred=model.predict_classes(scaled_input)\n",
    "    \n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_pred(model=iris_model,scaler=iris_scaler,input=json_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
